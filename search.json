[
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment #2."
  },
  {
    "objectID": "cv2.html",
    "href": "cv2.html",
    "title": "CV",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\nI have worked for the past few years and now I am back in school.\nOBJECTIVE\nSeeking a responsible and challenging career field that provides exposure to new ideas and stimulates personal and professional growth.\nSUMMARY\n-Ability to learn quickly -Motivated team player -Enthusiastic achiever -Energetic, Disciplined and loyal worker\nEDUCATION\n2011 - 2013 Collin County Community College, Plano, TX 75023 Associate of Applied Science Degree in Graphic Design (Web Track) Overall GPA 3.872/4.00\n2003 - 2005 Collin County Community College, Plano, TX 75023 Associate of Applied Science Degree in Graphic Design Overall GPA 3.83/4.00\n1998 - 2000 University of North Texas, Denton, TX 76201  BS in Business Computer Information Systems  Overall GPA 3.489/4.00\n1996 - 1998 Collin County Community College, Plano, TX 75023 Associate Degree in Associate of Arts GPA 3.627/4.00 - Graduated with honors (Cum Laude)\n\nSKILLS\nLANGUAGE . Fluent in English, Malay, Mandarin and Chinese dialects (Teochew, Hokkien and Cantonese)"
  },
  {
    "objectID": "final.html",
    "href": "final.html",
    "title": "Final Project",
    "section": "",
    "text": "Demo-1.html\nDemo-2.html\n\nScripts:\n\nAnalysis File\n\n\n# EPPS 6302 Group Project Analysis File\n# By: Genna Campain, Ryan Gordon, Su Lin Goh, Michelle Kim\n\n# Library\nlibrary(spotifyr)\nlibrary(magrittr)\nlibrary(geniusr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(stringr)\nlibrary(readr)\n\n# Data loading and editing\nload(\"~/Desktop/Fall 2022/Data methods/Group Project/lyrics_data.Rda\")\nfinal_df$num &lt;- seq.int(from = 1, to = 250)\nfinal_df$year &lt;- 2018\nfinal_df$year[which(final_df$num &gt; 50 &amp; final_df$num &lt; 101)] &lt;- 2019\nfinal_df$year[which(final_df$num &gt; 100 &amp; final_df$num &lt; 151)] &lt;- 2020\nfinal_df$year[which(final_df$num &gt; 150 &amp; final_df$num &lt; 201)] &lt;- 2021\nfinal_df$year[which(final_df$num &gt; 200)] &lt;- 2022\n\n\n# Paste all lyrics for year into one line of dataframe\ndf2 &lt;- final_df %&gt;%\n  group_by(year) %&gt;%\n  summarise(col = paste(lyrics, collapse=\" \"))\n\n# Text analysis\nsp_stop_words &lt;- spanish_stopwords &lt;- read.table(\"~/Desktop/Fall 2022/Data methods/Group Project/spanish_stopwords.txt\", quote=\"\\\"\", comment.char=\"\") %&gt;%\n  rename(word = V1)\ncount_result &lt;- data.frame()\nafinn_result &lt;- data.frame()\nbing_and_nrc_result &lt;- data.frame()\nfor(j in 2018:2022) {\n  text &lt;- df2$col[which(df2$year == j)]\n  tibble &lt;- tibble(line = 1, text = text)\n  tidy_lyrics &lt;- unnest_tokens(tibble, word, text)\n  data(stop_words)\n  tidy_lyrics &lt;- tidy_lyrics %&gt;%\n    anti_join(stop_words) %&gt;%\n    anti_join(sp_stop_words)\n  # Count of common words\n  count &lt;- tidy_lyrics %&gt;%\n    count(word, sort = TRUE) %&gt;%\n    mutate(year = j)\n  count_result &lt;- rbind(count_result, count)\n  # Sentiment analysis using afinn\n  afinn &lt;- get_sentiments(\"afinn\")\n  afinn_text &lt;- tidy_lyrics %&gt;% \n    inner_join(afinn) %&gt;% \n    summarise(sentiment = sum(value)) %&gt;% \n    mutate(method = \"AFINN\") %&gt;%\n    mutate(year = j)\n  afinn_result &lt;- rbind(afinn_result, afinn_text)\n  # Sentiment analysis using bing and nrc\n  bing_and_nrc &lt;- bind_rows(\n    tidy_lyrics %&gt;% \n      inner_join(get_sentiments(\"bing\")) %&gt;%\n      mutate(method = \"Bing et al.\"),\n    tidy_lyrics %&gt;% \n      inner_join(get_sentiments(\"nrc\") %&gt;% \n                   filter(sentiment %in% c(\"positive\", \n                                           \"negative\"))\n      ) %&gt;%\n      mutate(method = \"NRC\")) %&gt;%\n    count(method, sentiment) %&gt;%\n    pivot_wider(names_from = sentiment,\n                values_from = n,\n                values_fill = 0) %&gt;% \n    mutate(sentiment = positive - negative) %&gt;%\n    mutate(year = j)\n  bing_and_nrc_result &lt;- rbind(bing_and_nrc_result, bing_and_nrc)\n}\n\n# Pull top 100 words of each year\ntop100words &lt;- count_result %&gt;% \n  group_by(year) %&gt;% \n  mutate(rown = row_number()) %&gt;%\n  ungroup()\ntop100words &lt;- top100words[which(top100words$rown &lt;= 100), ]\n\n## Replace profanity with placeholder letters\ntop100words$word[top100words$word == 'shit'] &lt;- 's***'\ntop100words$word[top100words$word == 'nigga'] &lt;- 'n****'\ntop100words$word[top100words$word == 'niggas'] &lt;- 'n*****'\ntop100words$word[top100words$word == 'bitch'] &lt;- 'b****'\ntop100words$word[top100words$word == 'fuck'] &lt;- 'f***'\ntop100words$word[top100words$word == 'ass'] &lt;- 'a**'\ntop100words$word[top100words$word == 'pussy'] &lt;- 'p****'\ntop100words$word[top100words$word == 'fuckin'] &lt;- 'f*****'\ntop100words$word[top100words$word == 'mothafuckin'] &lt;- 'm****f*****'\n\n## Word cloud with top 50 words\nlibrary(wordcloud2)\nlibrary(htmlwidgets)\nwords2018 &lt;- top100words[which(top100words$year == 2018), ] %&gt;%\n  select(word, n)\nwords2019 &lt;- top100words[which(top100words$year == 2019), ] %&gt;%\n  select(word, n)\nwords2020 &lt;- top100words[which(top100words$year == 2020), ] %&gt;%\n  select(word, n)\nwords2021 &lt;- top100words[which(top100words$year == 2021), ] %&gt;%\n  select(word, n)\nwords2022 &lt;- top100words[which(top100words$year == 2022), ] %&gt;%\n  select(word, n)\nhw1 = wordcloud2(words2018, shape = 'triangle')\nhw2 = wordcloud2(words2019, shape = 'triangle')\nhw3 = wordcloud2(words2020, shape = 'triangle')\nhw4 = wordcloud2(words2021, shape = 'triangle')\nhw5 = wordcloud2(words2022, shape = 'triangle')\nsaveWidget(hw1,\"1.html\",selfcontained = F)\nwebshot::webshot(\"1.html\",\"1.png\",vwidth = 1992, vheight = 1744, delay =10)\nsaveWidget(hw2,\"2.html\",selfcontained = F)\nwebshot::webshot(\"2.html\",\"2.png\",vwidth = 1992, vheight = 1744, delay =10)\nsaveWidget(hw3,\"3.html\",selfcontained = F)\nwebshot::webshot(\"3.html\",\"3.png\",vwidth = 1992, vheight = 1744, delay =10)\nsaveWidget(hw4,\"4.html\",selfcontained = F)\nwebshot::webshot(\"4.html\",\"4.png\",vwidth = 1992, vheight = 1744, delay =10)\nsaveWidget(hw5,\"5.html\",selfcontained = F)\nwebshot::webshot(\"5.html\",\"5.png\",vwidth = 1992, vheight = 1744, delay =10)\n\n# Identify which words are not being matched with lexicon\ntext &lt;- df2$col\ntibble &lt;- tibble(line = 1, text = text)\ntidy_lyrics &lt;- unnest_tokens(tibble, word, text)\ndata(stop_words)\ntidy_lyrics &lt;- tidy_lyrics %&gt;%\n  anti_join(stop_words) %&gt;%\n  anti_join(sp_stop_words)\n# Pull out words not matched to afinn\nafinn &lt;- get_sentiments(\"afinn\")\nnot_matched_afinn &lt;- tidy_lyrics %&gt;% \n  anti_join(afinn) %&gt;%\n    unique()\n# Pull out words not matched to bing and nrc\nnot_matched_bing &lt;- tidy_lyrics %&gt;% \n    anti_join(get_sentiments(\"bing\")) %&gt;%\n    unique()\nnot_matched_nrc &lt;- tidy_lyrics %&gt;% \n    anti_join(get_sentiments(\"nrc\"))%&gt;%\n    unique()\n# Spanish language analysis\npos_sp &lt;- read_csv(\"isol/positivas_mejorada.csv\", col_names = FALSE) %&gt;%\n  mutate(score = 1)\nneg_sp &lt;- read_csv(\"isol/negativas_mejorada.csv\", col_names = FALSE) %&gt;%\n  mutate(score = -1)\nsp_sent &lt;- rbind(pos_sp, neg_sp) %&gt;%\n  rename(word = X1)\n# sp_text &lt;- tidy_lyrics %&gt;% \n  # inner_join(sp_sent) %&gt;% \n  # summarise(sentiment = sum(score))\n\n## Save as CSV files\nsetwd(\"~/Desktop/Fall 2022/Data methods/Group Project\")\nwrite.csv(not_matched_afinn,\"Not matched afinn.csv\", row.names = FALSE)\nwrite.csv(not_matched_bing_and_nrc,\"Not matched bingnrc.csv\", row.names = FALSE)\n\n# Combine bing, spanish and self-created sentiment values\nbing_sent &lt;- get_sentiments(\"bing\") %&gt;%\n  rename(score = sentiment)\nbing_sent$score[bing_sent$score == 'positive'] &lt;- 1\nbing_sent$score[bing_sent$score == 'negative'] &lt;- -1\nour_sent &lt;- read_excel(\"Not matched bingnrc.xlsx\") %&gt;%\n  rename(score = \"...3\") %&gt;%\n  na.omit() %&gt;%\n  select(word, score)\nmix_sent &lt;- rbind(bing_sent, sp_sent) %&gt;%\n  rbind(our_sent)\nmix_sent$score &lt;- as.numeric(mix_sent$score)\n\nbing_sp_sent &lt;- rbind(bing_sent, sp_sent)\nbing_sp_sent$score &lt;- as.numeric(bing_sp_sent$score)\n\n# Pull out words not matched to new lexicons\nnot_matched_spb &lt;- tidy_lyrics %&gt;% \n  anti_join(bing_sp_sent)%&gt;%\n  unique()\nnot_matched_mix &lt;- tidy_lyrics %&gt;% \n  anti_join(mix_sent)%&gt;%\n  unique()\n\n# Analyze with new values\nmix_result &lt;- data.frame()\nfor(j in 2018:2022) {\n  text &lt;- df2$col[which(df2$year == j)]\n  tibble &lt;- tibble(line = 1, text = text)\n  tidy_lyrics &lt;- unnest_tokens(tibble, word, text)\n  data(stop_words)\n  tidy_lyrics &lt;- tidy_lyrics %&gt;%\n    anti_join(stop_words) %&gt;%\n    anti_join(sp_stop_words)\n  # Sentiment analysis using self-created lexicon\n  mix_text &lt;- tidy_lyrics %&gt;% \n    inner_join(mix_sent) %&gt;% \n    summarise(sentiment = sum(score)) %&gt;% \n    mutate(method = \"mixed\") %&gt;%\n    mutate(year = j)\n  mix_result &lt;- rbind(mix_result, mix_text)\n}\n\nbing_sp_result &lt;- data.frame()\nfor(j in 2018:2022) {\n  text &lt;- df2$col[which(df2$year == j)]\n  tibble &lt;- tibble(line = 1, text = text)\n  tidy_lyrics &lt;- unnest_tokens(tibble, word, text)\n  data(stop_words)\n  tidy_lyrics &lt;- tidy_lyrics %&gt;%\n    anti_join(stop_words) %&gt;%\n    anti_join(sp_stop_words)\n  # Sentiment analysis using self-created lexicon\n  bing_sp_text &lt;- tidy_lyrics %&gt;% \n    inner_join(bing_sp_sent) %&gt;% \n    summarise(sentiment = sum(score)) %&gt;% \n    mutate(method = \"bing_sp\") %&gt;%\n    mutate(year = j)\n  bing_sp_result &lt;- rbind(bing_sp_result, bing_sp_text)\n}\n\n## Create visualizations\nbing &lt;- bing_and_nrc_result[which(bing_and_nrc_result$method == \"Bing et al.\"), ] %&gt;%\n  select(sentiment, year) %&gt;%\n  mutate(method = \"Bing\") \nnrc &lt;- bing_and_nrc_result[which(bing_and_nrc_result$method == \"NRC\"), ]  %&gt;%\n  select(sentiment, year) %&gt;%\n  mutate(method = \"NRC\")\nafinn &lt;- afinn_result %&gt;%\n  select(sentiment, year, method)\nbing_sp &lt;- bing_sp_result %&gt;%\n  select(sentiment, year) %&gt;%\n  mutate(method = \"Bing_sp\")\nmix &lt;- mix_result %&gt;%\n  select(sentiment, year) %&gt;%\n  mutate(method = \"Mix\")\nbig_df &lt;- rbind(bing, nrc) %&gt;%\n  rbind(afinn) %&gt;%\n  rbind(bing_sp) %&gt;%\n  rbind(mix) %&gt;%\n  rename(Year = year, Method = method)\nbig_df$Year &lt;- as.factor(big_df$Year)\nsave(big_df, file=\"all sentiments combined.Rda\")\n\n### library(ggplot2)\n### library(RColorBrewer)\n### library(ggthemes)\nggplot(big_df,\n       aes(x = Year,\n           y = sentiment,\n           fill = Method)) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Sentiment Score\") + \n  scale_fill_brewer(palette = \"Set1\")\n\n# Calculate summary statistics for audio analysis\nload(\"~/Desktop/Fall 2022/Data methods/Group Project/playlist audio features.Rda\")\nsum_songs_ana &lt;- songs_ana %&gt;%\n  group_by(year) %&gt;%\n  summarise(m_dance = mean(danceability), \n            m_valence = mean(valence),\n            m_acoustic = mean(acousticness))\nsum_songs_ana$year &lt;- as.factor(sum_songs_ana$year)\n\nsongs_ana$year &lt;- as.factor(songs_ana$year)\n\nggplot(songs_ana, aes(valence, fill = year)) + \n  geom_density() +\n  facet_grid(year ~ .) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Valence\", y = \"Density\")\n\nggplot(songs_ana, aes(danceability, fill = year)) + \n  geom_density() +\n  facet_grid(year ~ .) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Danceability\", y = \"Density\")\n\nggplot(songs_ana, aes(acousticness, fill = year)) + \n  geom_density() +\n  facet_grid(year ~ .) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Acousticness\", y = \"Density\")\n\ndf1 &lt;- sum_songs_ana %&gt;%\n  select(year, m_dance) %&gt;%\n  mutate(Characteristic = \"Danceability\") %&gt;%\n  rename(Value = m_dance)\ndf2 &lt;- sum_songs_ana %&gt;%\n  select(year, m_valence) %&gt;%\n  mutate(Characteristic = \"Valence\") %&gt;%\n  rename(Value = m_valence)\ndf3 &lt;- sum_songs_ana %&gt;%\n  select(year, m_acoustic) %&gt;%\n  mutate(Characteristic = \"Acousticness\") %&gt;%\n  rename(Value = m_acoustic)\nsum_songs_ana &lt;- rbind(df1, df2) %&gt;%\n  rbind(df3)\nsum_songs_ana$Year &lt;- as.factor(sum_songs_ana$year)\n\nggplot(sum_songs_ana,\n       aes(x = Year,\n           y = Value,\n           fill = Characteristic)) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Value\") + \n  scale_fill_brewer(palette = \"Set1\")\n\n# Unique words from each year\ncount_result &lt;- data.frame()\nfor(j in 2018:2022) {\n  text &lt;- df2$col[which(df2$year == j)]\n  tibble &lt;- tibble(line = 1, text = text)\n  tidy_lyrics &lt;- unnest_tokens(tibble, word, text)\n  data(stop_words)\n  tidy_lyrics &lt;- tidy_lyrics %&gt;%\n    anti_join(stop_words) %&gt;%\n    anti_join(sp_stop_words)\n\n  # Count of common words\n  count &lt;- tidy_lyrics %&gt;%\n    count(word, sort = TRUE) %&gt;%\n    mutate(year = j)\n  count_result &lt;- rbind(count_result, count)\n}\nwords2018 &lt;- count_result[which(count_result$year == 2018), ] %&gt;%\n  select(word)\nwords2019 &lt;- count_result[which(count_result$year == 2019), ] %&gt;%\n  select(word)\nwords2020 &lt;- count_result[which(count_result$year == 2020), ] %&gt;%\n  select(word)\nwords2021 &lt;- count_result[which(count_result$year == 2021), ] %&gt;%\n  select(word)\nwords2022 &lt;- count_result[which(count_result$year == 2022), ] %&gt;%\n  select(word)\n\nu_words2020 &lt;- words2020 %&gt;%\n  anti_join(words2018) %&gt;%\n  anti_join(words2019) %&gt;%\n  anti_join(words2021) %&gt;%\n  anti_join(words2022) %&gt;%\n  inner_join(count_result[which(count_result$year == 2020), ]) %&gt;%\n  select(word, n)\nu_words2020 &lt;- u_words2020[1:100,]\nu_words2020$word[u_words2020$word == 'shit'] &lt;- 's***'\nu_words2020$word[u_words2020$word == 'nigga'] &lt;- 'n****'\nu_words2020$word[u_words2020$word == 'niggas'] &lt;- 'n*****'\nu_words2020$word[u_words2020$word == 'bitch'] &lt;- 'b****'\nu_words2020$word[u_words2020$word == 'fuck'] &lt;- 'f***'\nu_words2020$word[u_words2020$word == 'ass'] &lt;- 'a**'\nu_words2020$word[u_words2020$word == 'pussy'] &lt;- 'p****'\nu_words2020$word[u_words2020$word == 'fuckin'] &lt;- 'f*****'\nu_words2020$word[u_words2020$word == 'mothafuckin'] &lt;- 'm****f*****'\nhw7 = wordcloud2(u_words2020, shape = 'triangle')\nsaveWidget(hw7,\"7.html\",selfcontained = F)\nwebshot::webshot(\"7.html\",\"7.png\",vwidth = 1992, vheight = 1744, delay =10)\n\n\n\n# Analyzing covid-specific songs\ntext &lt;- final_df2$col\ntibble &lt;- tibble(line = 1, text = text)\ntidy_lyrics &lt;- unnest_tokens(tibble, word, text)\ndata(stop_words)\ntidy_lyrics &lt;- tidy_lyrics %&gt;%\n  anti_join(stop_words)\n# Count of common words\ncount &lt;- tidy_lyrics %&gt;%\n  count(word, sort = TRUE)\n# Sentiment analysis using afinn\nafinn &lt;- get_sentiments(\"afinn\")\nafinn_text &lt;- tidy_lyrics %&gt;% \n  inner_join(afinn) %&gt;% \n  summarise(sentiment = sum(value)) %&gt;% \n  mutate(method = \"AFINN\")\n# Sentiment analysis using bing and nrc\nbing_and_nrc &lt;- bind_rows(\n  tidy_lyrics %&gt;% \n    inner_join(get_sentiments(\"bing\")) %&gt;%\n    mutate(method = \"Bing et al.\"),\n  tidy_lyrics %&gt;% \n    inner_join(get_sentiments(\"nrc\") %&gt;% \n                 filter(sentiment %in% c(\"positive\", \n                                         \"negative\"))\n    ) %&gt;%\n    mutate(method = \"NRC\")) %&gt;%\n  count(method, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n## word cloud\n## Replace profanity with placeholder letters\ncount$word[count$word == 'shit'] &lt;- 's***'\ncount$word[count$word == 'nigga'] &lt;- 'n****'\ncount$word[count$word == 'niggas'] &lt;- 'n*****'\ncount$word[count$word == 'bitch'] &lt;- 'b****'\ncount$word[count$word == 'fuck'] &lt;- 'f***'\ncount$word[count$word == 'ass'] &lt;- 'a**'\ncount$word[count$word == 'pussy'] &lt;- 'p****'\ncount$word[count$word == 'fuckin'] &lt;- 'f*****'\ncount$word[count$word == 'mothafuckin'] &lt;- 'm****f*****'\nhw6 = wordcloud2(count, shape = 'triangle')\nsaveWidget(hw6,\"6.html\",selfcontained = F)\nwebshot::webshot(\"6.html\",\"6.png\",vwidth = 1992, vheight = 1744, delay =10)\n\n# Analyze counts of words not being matched\nnot_matched_d &lt;- c(\"AFINN\", 5846, \"Bing\", 5680, \"NRC\", 5421, \"Bing_sp\", 5502, \"Mix\", 4618)\nnot_matched &lt;- matrix(not_matched_d, nrow = 5, ncol = 2, byrow = TRUE) %&gt;%\n  as.data.frame()\ncolnames(not_matched) &lt;- c(\"Lexicon\", \"Unmatched\")\nnot_matched$total &lt;- 37093\nnot_matched$Unmatched &lt;- as.numeric(not_matched$Unmatched)\nnot_matched$pct_unmatched &lt;- (not_matched$Unmatched/not_matched$total)*100\nggplot(not_matched,\n       aes(x = Lexicon,\n           y = pct_unmatched,\n           fill = Lexicon)) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge\", show.legend = FALSE) +\n  theme_minimal() +\n  labs(x = \"Lexicon\", y = \"Percent of Words Unmatched\") + \n  scale_fill_brewer(palette = \"Set1\")\n\n\nProject RMD\n\n\ntitle: \"Data Methods Group Project\"\nauthor: \"Genna Campain, Ryan Gordon, Su Lin Goh, Michelle Kim\"\ndate: \"9/10/2022\"\noutput: html_document\n---\n\n# Scrape and edit top songs lists\nLoad packages\nlibrary(spotifyr)\nlibrary(magrittr)\nlibrary(geniusr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(stringr)\n# Access token\nid &lt;- \"9bd81604c1724db4b9ad68f64dd7fbfe\"\nsecret &lt;- \"791feacce6c143f5bf2ecb23d517e055\"\nSys.setenv(SPOTIFY_CLIENT_ID = id)\nSys.setenv(SPOTIFY_CLIENT_SECRET = secret)\naccess_token &lt;- get_spotify_access_token()\ntoken &lt;- \"QPPNNpRdh3O3T-e7LbSGTeYzIefKe1ZL6CeoR26DKwRxdMbln6audf3TNViNVhEG\"\nSys.setenv(GENIUS_API_TOKEN = token)\n#Scraping top 100 songs of the year playlists\nyear_id &lt;- data.frame(id = c(\"37i9dQZF1DXe2bobNYDtW8\", \"37i9dQZF1DWVRSukIED0e9\", \"37i9dQZF1DX7Jl5KP2eZaS\", \"5GhQiRkGuqzpWZSE7OU4Se\", \"37i9dQZF1DX18jTM2l2fJY\"), year = 2018:2022)\n\ntop_songs &lt;- data.frame()\nfor(j in 1:5) {\nplaylist_id &lt;- year_id[j, 1]\nyear &lt;- year_id[j, 2]\ntracks &lt;- get_playlist_tracks(\n    playlist_id = playlist_id,\n    fields = c(\"track.artists\", \"track.duration_ms\", \"track.explicit\", \"track.id\", \"track.name\", \"track.popularity\", \"track.album.name\", \"track.album.release.date\")\n) %&gt;%\n  mutate(year = year)\ntop_songs &lt;- rbind(top_songs, tracks)\n}\n#Subset to take top 50 songs by year, unpack artist names\ntop_songs &lt;- top_songs %&gt;% \n  group_by(year) %&gt;% \n  mutate(rown = row_number()) %&gt;%\n  ungroup()\ntop_songs &lt;- top_songs[which(top_songs$rown &lt;= 50), ]\n\nartists &lt;- top_songs$track.artists\nartists2 &lt;- matrix(0, 0, nrow = 250, ncol = 1)\nfor(j in 1:250){ \n     df &lt;- artists[[j]]\n     name &lt;- df$name\n     artists2[j] &lt;- name\n}\ntop_songs &lt;- cbind(top_songs, artists2)\nsonginfo &lt;- select(top_songs, c(track.name, artists2, year)) %&gt;%\n  rename(track.artists = artists2)\n# Scrape song lyrics\nEdit song and artist names to match Genius website\nsonginfo$track.name &lt;- gsub(\"\\\\s*\\\\([^\\\\)]+\\\\)\",\"\",as.character(songinfo$track.name))\nsonginfo2018 &lt;- songinfo[which(songinfo$year == 2018), ]\nsonginfo2019 &lt;- songinfo[which(songinfo$year == 2019), ]\nsonginfo2020 &lt;- songinfo[which(songinfo$year == 2020), ]\nsonginfo2021 &lt;- songinfo[which(songinfo$year == 2021), ]\nsonginfo2022 &lt;- songinfo[which(songinfo$year == 2022), ]\n\n\n# 2018\nsonginfo2018[19, 1] &lt;- \"LOVE.\"\nsonginfo2018[15, 2] &lt;- \"Lil Baby and Drake\"\nsonginfo2018[6, 2] &lt;- \"Juice wrld\"\nsonginfo2018[13, 1] &lt;- \"happier\"\nsonginfo2018[17, 2] &lt;- \"Offset and Metro Boomin\"\nsonginfo2018[21, 2] &lt;- \"Anne marie\"\nsonginfo2018[32, 2] &lt;- \"Dynoro and gigi dagostino\"\nsonginfo2018[33, 2] &lt;- \"G Eazy and Halsey\"\nsonginfo2018[33, 1] &lt;- \"Him and I\"\nsonginfo2018[36, 1] &lt;- \"Te bote\"\nsonginfo2018[39, 1] &lt;- \"1 2 3\"\nsonginfo2018[39, 2] &lt;- \"Sofia Reyes\"\nsonginfo2018[41, 2] &lt;- \"Hailee Steinfeld and Alesso\"\nsonginfo2018[48, 1] &lt;- \"Dejala Que Vuelva\"\n# 2019\nsonginfo2019[3, 2] &lt;- \"Shawn Mendes and Camila Cabello\"\nsonginfo2019[3, 1] &lt;- \"Senorita\"\nsonginfo2019[5, 1] &lt;- \"Sunflower\"\nsonginfo2019[44, 1] &lt;- \"10000 hours\"\nsonginfo2019[44, 2] &lt;- \"Dan shay\"\n# 2020\nsonginfo2020[21, 2] &lt;- \"Jawsh 685 and Jason Derulo\"\nsonginfo2020[21, 1] &lt;- \"Savage love laxed siren beat\"\nsonginfo2020[26, 1] &lt;- \"Senorita\"\nsonginfo2020[26, 2] &lt;- \"Shawn Mendes and Camila Cabello\"\nsonginfo2020[38, 1] &lt;- \"Sunflower\"\nsonginfo2020[39, 1] &lt;- \"Hawai\"\nsonginfo2020[42, 1] &lt;- \"ritmo bad boys for life\"\nsonginfo2020[42, 2] &lt;- \"The black eyed peas and j balvin\"\nsonginfo2020[47, 2] &lt;- \"Ariana Grande and Justin Bieber\"\n\n# 2021\nsonginfo2021[2, 1] &lt;- \"Montero Call Me by Your Name\"\nsonginfo2021[10, 1] &lt;- \"Beggin\"\nsonginfo2021[10, 2] &lt;- \"Maneskin\"\nsonginfo2021[12, 1] &lt;- \"Dakiti\"\nsonginfo2021[17, 2] &lt;- \"Silk Sonic\"\nsonginfo2021[21, 2] &lt;- \"Tiesto\"\nsonginfo2021[25, 2] &lt;- \"Riton and Nightcrawlers\"\nsonginfo2021[25, 1] &lt;- \"Friday dopamine re edit\"\nsonginfo2021[26, 1] &lt;- \"telepatia\"\nsonginfo2021[33, 2] &lt;- \"Myke Towers and Juhn\"\nsonginfo2021[34, 2] &lt;- \"Maneskin\"\nsonginfo2021[46, 1] &lt;- \"Que Mas Pues\"\nsonginfo2021[48, 1] &lt;- \"34 35\"\nsonginfo2021[50, 1] &lt;- \"Pareja Del Ano\"\nsonginfo2021[50, 2] &lt;- \"Sebastian yatra and Myke Towers\"\n\n# 2022\nsonginfo2022[5, 1] &lt;- \"Titi Me Pregunto\"\nsonginfo2022[7, 1] &lt;- \"Enemy\"\nsonginfo2022[8, 1] &lt;- \"quevedo bzrp music sessions vol 52\"\nsonginfo2022[8, 2] &lt;- \"Bizarrap and quevedo\"\nsonginfo2022[10, 1] &lt;- \"Running up that hill a deal with god\"\nsonginfo2022[19, 2] &lt;- \"Elley Duhe\"\nsonginfo2022[22, 2] &lt;- \"Rauw alejandro and chencho corleone\"\nsonginfo2022[29, 2] &lt;- \"Lost frequencies and calum scott\"\nsonginfo2022[36, 1] &lt;- \"I Aint Worried\"\nsonginfo2022[39, 1] &lt;- \"Una Noche en Medellin\"\nsonginfo2022[42, 2] &lt;- \"Bad Bunny &amp; Rauw Alejandro\"\nsonginfo2022[44, 2] &lt;- \"Tiesto\"\nLoop for scraping (for some reason loop randomly breaks if too many numbers)\n# 2018\nfinal_df &lt;- data.frame()\nfor(j in 48:50) {\n  artist_name &lt;- songinfo2018[j, 2]\n  song_title &lt;- songinfo2018[j, 1]\n  df &lt;- get_lyrics_search(artist_name = artist_name, song_title = song_title)\n  df &lt;- df$line\n  df &lt;- as.vector(df)\n  df&lt;- paste(df, collapse = \" \") \n  final_df &lt;- rbind(final_df, df)\n}\n\n# 2019\nfor(j in 48:50) {\n  artist_name &lt;- songinfo2019[j, 2]\n  song_title &lt;- songinfo2019[j, 1]\n  df &lt;- get_lyrics_search(artist_name = artist_name, song_title = song_title)\n  df &lt;- df$line\n  df &lt;- as.vector(df)\n  df&lt;- paste(df, collapse = \" \") \n  final_df &lt;- rbind(final_df, df)\n}\n\n# 2020\nfor(j in 47:50) {\n  artist_name &lt;- songinfo2020[j, 2]\n  song_title &lt;- songinfo2020[j, 1]\n  df &lt;- get_lyrics_search(artist_name = artist_name, song_title = song_title)\n  df &lt;- df$line\n  df &lt;- as.vector(df)\n  df&lt;- paste(df, collapse = \" \") \n  final_df &lt;- rbind(final_df, df)\n}\n\n# 2021\nfor(j in 48:50) {\n  artist_name &lt;- songinfo2021[j, 2]\n  song_title &lt;- songinfo2021[j, 1]\n  df &lt;- get_lyrics_search(artist_name = artist_name, song_title = song_title)\n  df &lt;- df$line\n  df &lt;- as.vector(df)\n  df&lt;- paste(df, collapse = \" \")\n  final_df &lt;- rbind(final_df, df)\n}\n\n# 2022\nfinal_df &lt;- data.frame()\nfor(j in 44:50) {\n  artist_name &lt;- songinfo2022[j, 2]\n  song_title &lt;- songinfo2022[j, 1]\n  df &lt;- get_lyrics_search(artist_name = artist_name, song_title = song_title)\n  df &lt;- df$line\n  df &lt;- as.vector(df)\n  df&lt;- paste(df, collapse = \" \")\n  final_df &lt;- rbind(final_df, df)\n}\n\ncolnames(final_df)[1] &lt;- \"lyrics\"\nsetwd(\"~/Desktop/Fall 2022/Data methods/Group Project\")\nsave(final_df, file=\"lyrics_data.Rda\")\nLabel column and add year\nload(\"~/Desktop/Fall 2022/Data methods/Group Project/lyrics_data.Rda\")\nfinal_df$num &lt;- seq.int(from = 1, to = 250)\nfinal_df$year &lt;- 2018\nfinal_df$year[which(final_df$num &gt; 50 &amp; final_df$num &lt; 101)] &lt;- 2019\nfinal_df$year[which(final_df$num &gt; 100 &amp; final_df$num &lt; 151)] &lt;- 2020\nfinal_df$year[which(final_df$num &gt; 150 &amp; final_df$num &lt; 201)] &lt;- 2021\nfinal_df$year[which(final_df$num &gt; 200)] &lt;- 2022\nPaste all lyrics for year into one line of dataframe\nhttps://stackoverflow.com/questions/54805201/how-to-paste-all-string-values-in-a-column-together-as-one \n# insert year here\ndf2 &lt;- final_df %&gt;%\n  group_by(year) %&gt;%\n  summarise(col = paste(lyrics, collapse=\" \"))\ndf2 &lt;- df2 %&gt;%\n  ungroup()\nwrite.csv(df2,\"lyrics_oneline.csv\", row.names = FALSE)\nText analysis\ncount_result &lt;- data.frame()\nafinn_result &lt;- data.frame()\nbing_and_nrc_result &lt;- data.frame()\nfor(j in 2018:2021) {\ntext &lt;- df2$col[which(df2$year == j)]\ntibble &lt;- tibble(line = 1, text = text)\ntidy_lyrics &lt;- unnest_tokens(tibble, word, text)\ndata(stop_words)\ntidy_lyrics &lt;- tidy_lyrics %&gt;%\n  anti_join(stop_words)\n# Count of common words\ncount &lt;- tidy_lyrics %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  mutate(year = j)\ncount_result &lt;- rbind(count_result, count)\n# Sentiment analysis using afinn\nafinn &lt;- get_sentiments(\"afinn\")\nafinn_text &lt;- tidy_lyrics %&gt;% \n  inner_join(afinn) %&gt;% \n  summarise(sentiment = sum(value)) %&gt;% \n  mutate(method = \"AFINN\") %&gt;%\n  mutate(year = j)\nafinn_result &lt;- rbind(afinn_result, afinn_text)\n# Sentiment analysis using bing and nrc\nbing_and_nrc &lt;- bind_rows(\n  tidy_lyrics %&gt;% \n    inner_join(get_sentiments(\"bing\")) %&gt;%\n    mutate(method = \"Bing et al.\"),\n   tidy_lyrics %&gt;% \n    inner_join(get_sentiments(\"nrc\") %&gt;% \n                 filter(sentiment %in% c(\"positive\", \n                                         \"negative\"))\n    ) %&gt;%\n    mutate(method = \"NRC\")) %&gt;%\n  count(method, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative) %&gt;%\n  mutate(year = j)\nbing_and_nrc_result &lt;- rbind(bing_and_nrc_result, bing_and_nrc)\n}\n\nGet audio features\nyear_id &lt;- data.frame(id = c(\"37i9dQZF1DXe2bobNYDtW8\", \"37i9dQZF1DWVRSukIED0e9\", \"37i9dQZF1DX7Jl5KP2eZaS\", \"5GhQiRkGuqzpWZSE7OU4Se\", \"37i9dQZF1DX18jTM2l2fJY\"), year = 2018:2022)\n\nsongs_ana &lt;- data.frame()\nfor(j in 1:5) {\nplaylist_id &lt;- year_id[j, 1]\nyear &lt;- year_id[j, 2]\ntracks &lt;- get_playlist_audio_features(\n  \"spotify\",\n  playlist_id) %&gt;%\n  mutate(year = year)\nsongs_ana &lt;- rbind(songs_ana, tracks)\n}\nsongs_ana &lt;- inner_join(songs_ana, top_songs, by = c(\"track.id\", \"year\"))\nsongs_ana &lt;- songs_ana %&gt;%\n  select(danceability, energy, key, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, year, track.name.y, track.id)\nsave(songs_ana, file=\"playlist audio features.Rda\")\n# write.csv(songs_ana,\"playlist audio features.csv\", row.names = FALSE)\nGet lyrics for songs specifically about COVID-19\ncovid_songs &lt;- read_excel(\"covid songs.xlsx\") %&gt;%\n  as.matrix()\nfinal_df2 &lt;- data.frame()\nfor(j in 7:23) {\n  artist_name &lt;-covid_songs[j, 2]\n  song_title &lt;- covid_songs[j, 1]\n  df &lt;- get_lyrics_search(artist_name = artist_name, song_title = song_title)\n  df &lt;- df$line\n  df &lt;- as.vector(df)\n  df&lt;- paste(df, collapse = \" \") \n  final_df2 &lt;- rbind(final_df2, df)\n}\ncolnames(final_df2) &lt;- \"lyrics\"\nfinal_df2 &lt;- final_df2 %&gt;%\n  summarise(col = paste(lyrics, collapse=\" \"))\nsave(final_df2, file=\"covid songs.Rda\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Su Goh Website",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\nThis is my first time using Rstudio."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Su Goh and this is my first semester at UT Dallas."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\nI have worked for the past few years and now I am back in school.\nOBJECTIVE\nSeeking a responsible and challenging career field that provides exposure to new ideas and stimulates personal and professional growth.\nSUMMARY\n-Ability to learn quickly -Motivated team player -Enthusiastic achiever -Energetic, Disciplined and loyal worker\nEDUCATION\n2011 - 2013 Collin County Community College, Plano, TX 75023 Associate of Applied Science Degree in Graphic Design (Web Track) Overall GPA 3.872/4.00\n2003 - 2005 Collin County Community College, Plano, TX 75023 Associate of Applied Science Degree in Graphic Design Overall GPA 3.83/4.00\n1998 - 2000 University of North Texas, Denton, TX 76201  BS in Business Computer Information Systems  Overall GPA 3.489/4.00\n1996 - 1998 Collin County Community College, Plano, TX 75023 Associate Degree in Associate of Arts GPA 3.627/4.00 - Graduated with honors (Cum Laude)\n\nSKILLS\nLANGUAGE . Fluent in English, Malay, Mandarin and Chinese dialects (Teochew, Hokkien and Cantonese)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\nI have a blog and you are welcome to my blog."
  }
]